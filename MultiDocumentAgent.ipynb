{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'YOUR_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setuping an agent over 3 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper : metagpt.pdf\n",
      "Getting tools for paper : longlora.pdf\n",
      "Getting tools for paper : selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper : {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x1600ee990>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x1600f3ce0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x16009a000>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x16006b0e0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x160090830>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x16017adb0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
    "initial_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    "    \n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA,and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in the experiments is the PG19 dataset.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results demonstrate that the proposed models achieve state-of-the-art performance on various benchmarks, such as LongBench and LEval. These models have been fine-tuned to different context lengths, ranging from 8192 to 65536, and have shown competitive perplexity scores. Additionally, an efficiency analysis indicates that the models, particularly with the S2-Attn mechanism, reduce computational overhead and training hours significantly compared to full fine-tuning methods. The models exhibit promising performance in terms of efficiency and effectiveness across different evaluation contexts and benchmarks.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 dataset. The evaluation results show that the proposed models achieve state-of-the-art performance on benchmarks like LongBench and LEval. The models have been fine-tuned to different context lengths and have competitive perplexity scores. The efficiency analysis indicates that the models, especially with the S2-Attn mechanism, reduce computational overhead and training hours significantly compared to full fine-tuning methods. Overall, the models demonstrate promising performance in terms of efficiency and effectiveness across various evaluation contexts and benchmarks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA,\"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework designed to improve the quality and factuality of large language models by incorporating retrieval on demand and self-reflection mechanisms. It enables a single arbitrary language model to retrieve, generate, and evaluate text passages and its own outputs using reflection tokens. By allowing the language model to adjust its behavior during the inference phase, Self-RAG aims to enhance performance across various tasks. Additionally, the system evaluates the factual precision and usefulness of text outputs by assessing aspects such as factual relevance, supportiveness, and overall utility. Through a structured evaluation process involving instructions, evidence, and output sentences, Self-RAG seeks to enhance the quality and reliability of text generation models.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context sizes of pre-trained large language models with limited computation cost. It combines shifted sparse attention during training to approximate the standard self-attention pattern. This method allows for extending the context length of language models significantly while maintaining minimal accuracy compromise. Additionally, LongLoRA includes the Action Units Relation Transformer (ART) for modeling relations between facial action units and the Tampered AU Prediction (TAP) for tampering AU-related regions to improve model generalization. It achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, with the ART encoder capturing intra-face relations and the TAP process generating challenging pseudosamples for improved generalization.\n",
      "=== LLM Response ===\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework designed to enhance the quality and factuality of large language models by incorporating retrieval on demand and self-reflection mechanisms. It allows a language model to retrieve, generate, and evaluate text passages and its own outputs using reflection tokens. By enabling the model to adjust its behavior during the inference phase, Self-RAG aims to improve performance across various tasks. The system evaluates the factual precision and usefulness of text outputs by assessing aspects like factual relevance, supportiveness, and overall utility. Through a structured evaluation process involving instructions, evidence, and output sentences, Self-RAG aims to enhance the quality and reliability of text generation models.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context sizes of pre-trained large language models with minimal computation cost. It utilizes shifted sparse attention during training to approximate the standard self-attention pattern, allowing for significant extension of the context length of language models while maintaining minimal accuracy compromise. LongLoRA also incorporates the Action Units Relation Transformer (ART) for modeling relations between facial action units and the Tampered AU Prediction (TAP) for tampering AU-related regions to enhance model generalization. It achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, with the ART encoder capturing intra-face relations and the TAP process generating challenging pseudosamples for improved generalization.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting an agent over 11 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "paper_to_tools_dict = {}\n",
    "\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem )\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the Agent With Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tools = [t for paer in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='vector_tool_selfrag(query: str, page_numbers: Optional[List[str]] = None) -> str\\nUse to answer questions over the MetaGPT paper.\\n    \\n        Useful if you have specific questions over the MetaGPT paper.\\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\\n    \\n        Args:\\n            query (str): the string query to be embedded.\\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \\n                if we want to perform a vector search\\n                over all pages. Otherwise, filter by the set of specified pages.\\n        \\n        ', name='vector_tool_selfrag', fn_schema=<class 'llama_index.core.tools.utils.vector_tool_selfrag'>, return_direct=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[1].metadata # returns second most relevant object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset usedin MetaGPT and commpare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_selfrag with args: {\"query\": \"evaluation dataset\", \"page_numbers\": [\"6\"]}\n",
      "=== Function Output ===\n",
      "The evaluation dataset includes a factverification dataset about public health (PubHealth; Zhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams (ARC-Challenge; Clark et al. 2018).\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_selfrag with args: {\"query\": \"SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "SWE-Bench is a benchmark for software engineering tasks.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes a fact verification dataset about public health (PubHealth) and a multiple-choice reasoning dataset created from scientific exams (ARC-Challenge). On the other hand, SWE-Bench is a benchmark for software engineering tasks.\n",
      "The evaluation dataset used in MetaGPT includes a fact verification dataset about public health (PubHealth) and a multiple-choice reasoning dataset created from scientific exams (ARC-Challenge). On the other hand, SWE-Bench is a benchmark for software engineering tasks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used\"\n",
    "    \"in MetaGPT and commpare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLora, LoftQ),Analyze the approach in each paper first.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_selfrag with args: {\"query\": \"LongLora paper approach\"}\n",
      "=== Function Output ===\n",
      "The LongLora paper approach focuses on improving the factuality of language model outputs by addressing issues such as misinformation and incorrect advice. The method aims to enhance performance, factuality, and citation accuracy, while also acknowledging the potential for generating outputs that may not be fully supported by citations. The authors emphasize the importance of self-reflection and detailed attribution to assist users in verifying factual errors in the model's outputs.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_selfrag with args: {\"query\": \"LoftQ paper approach\"}\n",
      "=== Function Output ===\n",
      "The paper approach involves aiming to enhance the factuality of outputs generated by large language models, with a focus on improving performance, factuality, and citation accuracy. The method acknowledges the potential for generating outputs that may not be fully supported by citations and suggests that self-reflection and detailed attribution could assist users in identifying factual errors in the model's outputs.\n",
      "=== LLM Response ===\n",
      "The LongLora paper approach focuses on improving the factuality of language model outputs by addressing issues such as misinformation and incorrect advice. The method aims to enhance performance, factuality, and citation accuracy, while also acknowledging the potential for generating outputs that may not be fully supported by citations. The authors emphasize the importance of self-reflection and detailed attribution to assist users in verifying factual errors in the model's outputs.\n",
      "\n",
      "The LoftQ paper approach also aims to enhance the factuality of outputs generated by large language models, with a focus on improving performance, factuality, and citation accuracy. Similar to LongLora, this method acknowledges the potential for generating outputs that may not be fully supported by citations and suggests that self-reflection and detailed attribution could assist users in identifying factual errors in the model's outputs.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLora, LoftQ),\"\n",
    "    \"Analyze the approach in each paper first.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
